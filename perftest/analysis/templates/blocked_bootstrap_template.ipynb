{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa82717",
   "metadata": {},
   "source": [
    "# Blocked Bootstrap Analysis\n",
    "\n",
    "Resampling-based approach that preserves the batch structure by resampling entire batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae0c89",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (will be injected)\n",
    "base_csv_path = ''\n",
    "test_csv_path = ''\n",
    "base_name = ''\n",
    "test_name = ''\n",
    "device_pool = ''\n",
    "test_type = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfe5de",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f'Base: {base_name}')\n",
    "print(f'Test: {test_name}')\n",
    "print(f'Device Pool: {device_pool}')\n",
    "print(f'Test Type: {test_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed metrics from CSV (already extracted in parallel)\n",
    "base_df = pd.read_csv(base_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200574ff",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('BASE RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(base_df)}')\n",
    "print(f'Number of batches: {base_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(base_df.groupby('batch').size())\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TEST RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(test_df)}')\n",
    "print(f'Number of batches: {test_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(test_df.groupby('batch').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4be472",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].boxplot([base_df[base_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[0, 0].set_title(f'Base - Startup Latency by Batch')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].boxplot([test_df[test_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[0, 1].set_title(f'Test - Startup Latency by Batch')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].boxplot([base_df[base_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[1, 0].set_title(f'Base - Render Latency by Batch')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].boxplot([test_df[test_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[1, 1].set_title(f'Test - Render Latency by Batch')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab082f",
   "metadata": {},
   "source": [
    "## Blocked Bootstrap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c6241",
   "metadata": {},
   "outputs": [],
   "source": "def blocked_bootstrap(base_df, test_df, metric, n_bootstrap=10000, alpha=0.05):\n    base_batches = base_df['batch'].unique()\n    test_batches = test_df['batch'].unique()\n    bootstrap_diffs = []\n    \n    for _ in range(n_bootstrap):\n        base_sample_batches = np.random.choice(base_batches, size=len(base_batches), replace=True)\n        test_sample_batches = np.random.choice(test_batches, size=len(test_batches), replace=True)\n        base_sample = pd.concat([base_df[base_df['batch'] == b] for b in base_sample_batches])\n        test_sample = pd.concat([test_df[test_df['batch'] == b] for b in test_sample_batches])\n        diff = test_sample[metric].mean() - base_sample[metric].mean()\n        bootstrap_diffs.append(diff)\n    \n    bootstrap_diffs = np.array(bootstrap_diffs)\n    observed_diff = test_df[metric].mean() - base_df[metric].mean()\n    ci_lower = np.percentile(bootstrap_diffs, alpha/2 * 100)\n    ci_upper = np.percentile(bootstrap_diffs, (1 - alpha/2) * 100)\n    p_value = np.mean((bootstrap_diffs * observed_diff) < 0) * 2\n    \n    return {\n        'observed_diff': observed_diff,\n        'ci_lower': ci_lower,\n        'ci_upper': ci_upper,\n        'p_value': p_value,\n        'bootstrap_diffs': bootstrap_diffs\n    }\n\n# Calculate summary statistics including percentiles\ndef calculate_summary_stats(df, metric):\n    return {\n        'mean': df[metric].mean(),\n        'p50': df[metric].quantile(0.5),\n        'p90': df[metric].quantile(0.9),\n        'p95': df[metric].quantile(0.95),\n        'p99': df[metric].quantile(0.99)\n    }\n\n# Startup latency analysis\nstartup_boot = blocked_bootstrap(base_df, test_df, 'startup_latency_ms')\nbase_startup_stats = calculate_summary_stats(base_df, 'startup_latency_ms')\ntest_startup_stats = calculate_summary_stats(test_df, 'startup_latency_ms')\n\nprint('\\n' + '=' * 80)\nprint('BLOCKED BOOTSTRAP RESULTS - STARTUP LATENCY')\nprint('=' * 80)\nprint(f'\\nBase Statistics:')\nprint(f'  Mean:  {base_startup_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {base_startup_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {base_startup_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {base_startup_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {base_startup_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nTest Statistics:')\nprint(f'  Mean:  {test_startup_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {test_startup_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {test_startup_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {test_startup_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {test_startup_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nDifferences (Test - Base):')\nprint(f'  Mean:  {test_startup_stats[\"mean\"] - base_startup_stats[\"mean\"]:.2f} ms ({(test_startup_stats[\"mean\"] - base_startup_stats[\"mean\"])/base_startup_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  P50:   {test_startup_stats[\"p50\"] - base_startup_stats[\"p50\"]:.2f} ms ({(test_startup_stats[\"p50\"] - base_startup_stats[\"p50\"])/base_startup_stats[\"p50\"]*100:+.2f}%)')\nprint(f'  P90:   {test_startup_stats[\"p90\"] - base_startup_stats[\"p90\"]:.2f} ms ({(test_startup_stats[\"p90\"] - base_startup_stats[\"p90\"])/base_startup_stats[\"p90\"]*100:+.2f}%)')\nprint(f'  P95:   {test_startup_stats[\"p95\"] - base_startup_stats[\"p95\"]:.2f} ms ({(test_startup_stats[\"p95\"] - base_startup_stats[\"p95\"])/base_startup_stats[\"p95\"]*100:+.2f}%)')\nprint(f'  P99:   {test_startup_stats[\"p99\"] - base_startup_stats[\"p99\"]:.2f} ms ({(test_startup_stats[\"p99\"] - base_startup_stats[\"p99\"])/base_startup_stats[\"p99\"]*100:+.2f}%)')\n\nprint(f'\\nBootstrap Analysis (using mean):')\nprint(f'  Observed difference: {startup_boot[\"observed_diff\"]:.2f} ms ({startup_boot[\"observed_diff\"]/base_startup_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  95% CI: [{startup_boot[\"ci_lower\"]:.2f}, {startup_boot[\"ci_upper\"]:.2f}]')\nprint(f'  P-value: {startup_boot[\"p_value\"]:.4f}')\nprint(f'  Significant at α=0.05: {\"YES\" if startup_boot[\"p_value\"] < 0.05 else \"NO\"}')\n\n# Render latency analysis\nrender_boot = blocked_bootstrap(base_df, test_df, 'render_latency_ms')\nbase_render_stats = calculate_summary_stats(base_df, 'render_latency_ms')\ntest_render_stats = calculate_summary_stats(test_df, 'render_latency_ms')\n\nprint('\\n' + '=' * 80)\nprint('BLOCKED BOOTSTRAP RESULTS - RENDER LATENCY')\nprint('=' * 80)\nprint(f'\\nBase Statistics:')\nprint(f'  Mean:  {base_render_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {base_render_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {base_render_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {base_render_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {base_render_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nTest Statistics:')\nprint(f'  Mean:  {test_render_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {test_render_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {test_render_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {test_render_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {test_render_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nDifferences (Test - Base):')\nprint(f'  Mean:  {test_render_stats[\"mean\"] - base_render_stats[\"mean\"]:.2f} ms ({(test_render_stats[\"mean\"] - base_render_stats[\"mean\"])/base_render_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  P50:   {test_render_stats[\"p50\"] - base_render_stats[\"p50\"]:.2f} ms ({(test_render_stats[\"p50\"] - base_render_stats[\"p50\"])/base_render_stats[\"p50\"]*100:+.2f}%)')\nprint(f'  P90:   {test_render_stats[\"p90\"] - base_render_stats[\"p90\"]:.2f} ms ({(test_render_stats[\"p90\"] - base_render_stats[\"p90\"])/base_render_stats[\"p90\"]*100:+.2f}%)')\nprint(f'  P95:   {test_render_stats[\"p95\"] - base_render_stats[\"p95\"]:.2f} ms ({(test_render_stats[\"p95\"] - base_render_stats[\"p95\"])/base_render_stats[\"p95\"]*100:+.2f}%)')\nprint(f'  P99:   {test_render_stats[\"p99\"] - base_render_stats[\"p99\"]:.2f} ms ({(test_render_stats[\"p99\"] - base_render_stats[\"p99\"])/base_render_stats[\"p99\"]*100:+.2f}%)')\n\nprint(f'\\nBootstrap Analysis (using mean):')\nprint(f'  Observed difference: {render_boot[\"observed_diff\"]:.2f} ms ({render_boot[\"observed_diff\"]/base_render_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  95% CI: [{render_boot[\"ci_lower\"]:.2f}, {render_boot[\"ci_upper\"]:.2f}]')\nprint(f'  P-value: {render_boot[\"p_value\"]:.4f}')\nprint(f'  Significant at α=0.05: {\"YES\" if render_boot[\"p_value\"] < 0.05 else \"NO\"}')"
  },
  {
   "cell_type": "markdown",
   "id": "076fed17",
   "metadata": {},
   "source": [
    "## Bootstrap Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].hist(startup_boot['bootstrap_diffs'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(startup_boot['observed_diff'], color='red', linestyle='--', linewidth=2, label='Observed')\n",
    "axes[0].axvline(startup_boot['ci_lower'], color='green', linestyle='--', linewidth=1.5, label='95% CI')\n",
    "axes[0].axvline(startup_boot['ci_upper'], color='green', linestyle='--', linewidth=1.5)\n",
    "axes[0].axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[0].set_xlabel('Difference (ms)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Bootstrap Distribution - Startup Latency Difference')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(render_boot['bootstrap_diffs'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(render_boot['observed_diff'], color='red', linestyle='--', linewidth=2, label='Observed')\n",
    "axes[1].axvline(render_boot['ci_lower'], color='green', linestyle='--', linewidth=1.5, label='95% CI')\n",
    "axes[1].axvline(render_boot['ci_upper'], color='green', linestyle='--', linewidth=1.5)\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('Difference (ms)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Bootstrap Distribution - Render Latency Difference')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}