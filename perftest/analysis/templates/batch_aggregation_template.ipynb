{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7f29af",
   "metadata": {},
   "source": [
    "# Batch Aggregation Analysis\n",
    "\n",
    "Conservative statistical approach that aggregates measurements to batch-level means before performing hypothesis tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae53c0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (will be injected)\n",
    "base_csv_path = ''\n",
    "test_csv_path = ''\n",
    "base_name = ''\n",
    "test_name = ''\n",
    "device_pool = ''\n",
    "test_type = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b778d0c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f'Base: {base_name}')\n",
    "print(f'Test: {test_name}')\n",
    "print(f'Device Pool: {device_pool}')\n",
    "print(f'Test Type: {test_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed metrics from CSV (already extracted in parallel)\n",
    "base_df = pd.read_csv(base_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18bd24",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce336504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('BASE RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(base_df)}')\n",
    "print(f'Number of batches: {base_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(base_df.groupby('batch').size())\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TEST RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(test_df)}')\n",
    "print(f'Number of batches: {test_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(test_df.groupby('batch').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71560a6b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].boxplot([base_df[base_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[0, 0].set_title(f'Base - Startup Latency by Batch')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].boxplot([test_df[test_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[0, 1].set_title(f'Test - Startup Latency by Batch')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].boxplot([base_df[base_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[1, 0].set_title(f'Base - Render Latency by Batch')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].boxplot([test_df[test_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[1, 1].set_title(f'Test - Render Latency by Batch')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc315e2",
   "metadata": {},
   "source": [
    "## Batch Aggregation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15742",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_batch_stats = base_df.groupby('batch').agg({\n",
    "    'startup_latency_ms': ['mean', 'std', 'count'],\n",
    "    'render_latency_ms': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "test_batch_stats = test_df.groupby('batch').agg({\n",
    "    'startup_latency_ms': ['mean', 'std', 'count'],\n",
    "    'render_latency_ms': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "print('=' * 80)\n",
    "print('BATCH-LEVEL STATISTICS')\n",
    "print('=' * 80)\n",
    "print('\\nBase Run - Batch Means:')\n",
    "print(base_batch_stats)\n",
    "print('\\nTest Run - Batch Means:')\n",
    "print(test_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_startup_means = base_batch_stats['startup_latency_ms']['mean'].values\n",
    "test_startup_means = test_batch_stats['startup_latency_ms']['mean'].values\n",
    "base_render_means = base_batch_stats['render_latency_ms']['mean'].values\n",
    "test_render_means = test_batch_stats['render_latency_ms']['mean'].values\n",
    "\n",
    "def permutation_test(group1, group2, n_permutations=10000):\n",
    "    observed_diff = np.mean(group2) - np.mean(group1)\n",
    "    combined = np.concatenate([group1, group2])\n",
    "    n1 = len(group1)\n",
    "    perm_diffs = []\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_diff = np.mean(combined[n1:]) - np.mean(combined[:n1])\n",
    "        perm_diffs.append(perm_diff)\n",
    "    p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "    return observed_diff, p_value\n",
    "\n",
    "startup_diff, startup_pval = permutation_test(base_startup_means, test_startup_means)\n",
    "print('\\n' + '=' * 80)\n",
    "print('BATCH AGGREGATION RESULTS - STARTUP LATENCY')\n",
    "print('=' * 80)\n",
    "print(f'Base mean (batch averages): {np.mean(base_startup_means):.2f} ms')\n",
    "print(f'Test mean (batch averages): {np.mean(test_startup_means):.2f} ms')\n",
    "print(f'Difference: {startup_diff:.2f} ms ({startup_diff/np.mean(base_startup_means)*100:+.2f}%)')\n",
    "print(f'P-value (permutation test): {startup_pval:.4f}')\n",
    "print(f'Significant at α=0.05: {\"YES\" if startup_pval < 0.05 else \"NO\"}')\n",
    "\n",
    "render_diff, render_pval = permutation_test(base_render_means, test_render_means)\n",
    "print('\\n' + '=' * 80)\n",
    "print('BATCH AGGREGATION RESULTS - RENDER LATENCY')\n",
    "print('=' * 80)\n",
    "print(f'Base mean (batch averages): {np.mean(base_render_means):.2f} ms')\n",
    "print(f'Test mean (batch averages): {np.mean(test_render_means):.2f} ms')\n",
    "print(f'Difference: {render_diff:.2f} ms ({render_diff/np.mean(base_render_means)*100:+.2f}%)')\n",
    "print(f'P-value (permutation test): {render_pval:.4f}')\n",
    "print(f'Significant at α=0.05: {\"YES\" if render_pval < 0.05 else \"NO\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
