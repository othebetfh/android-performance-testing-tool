{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7f29af",
   "metadata": {},
   "source": [
    "# Batch Aggregation Analysis\n",
    "\n",
    "Conservative statistical approach that aggregates measurements to batch-level means before performing hypothesis tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae53c0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (will be injected)\n",
    "base_csv_path = ''\n",
    "test_csv_path = ''\n",
    "base_name = ''\n",
    "test_name = ''\n",
    "device_pool = ''\n",
    "test_type = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b778d0c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f'Base: {base_name}')\n",
    "print(f'Test: {test_name}')\n",
    "print(f'Device Pool: {device_pool}')\n",
    "print(f'Test Type: {test_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed metrics from CSV (already extracted in parallel)\n",
    "base_df = pd.read_csv(base_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18bd24",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce336504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('BASE RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(base_df)}')\n",
    "print(f'Number of batches: {base_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(base_df.groupby('batch').size())\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TEST RUN SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Total traces: {len(test_df)}')\n",
    "print(f'Number of batches: {test_df[\"batch\"].nunique()}')\n",
    "print('\\nTraces per batch:')\n",
    "print(test_df.groupby('batch').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71560a6b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].boxplot([base_df[base_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[0, 0].set_title(f'Base - Startup Latency by Batch')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].boxplot([test_df[test_df['batch'] == b]['startup_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[0, 1].set_title(f'Test - Startup Latency by Batch')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].boxplot([base_df[base_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(base_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(base_df['batch'].unique())])\n",
    "axes[1, 0].set_title(f'Base - Render Latency by Batch')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].boxplot([test_df[test_df['batch'] == b]['render_latency_ms'].values \n",
    "                     for b in sorted(test_df['batch'].unique())],\n",
    "                    labels=[f'Batch {b}' for b in sorted(test_df['batch'].unique())])\n",
    "axes[1, 1].set_title(f'Test - Render Latency by Batch')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc315e2",
   "metadata": {},
   "source": [
    "## Batch Aggregation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15742",
   "metadata": {},
   "outputs": [],
   "source": "base_batch_stats = base_df.groupby('batch').agg({\n    'startup_latency_ms': ['mean', 'std', 'count', lambda x: x.quantile(0.5), lambda x: x.quantile(0.9), lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)],\n    'render_latency_ms': ['mean', 'std', 'count', lambda x: x.quantile(0.5), lambda x: x.quantile(0.9), lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)]\n}).reset_index()\n\n# Rename the lambda columns to proper names\nbase_batch_stats.columns = ['batch', 'startup_mean', 'startup_std', 'startup_count', 'startup_p50', 'startup_p90', 'startup_p95', 'startup_p99',\n                             'render_mean', 'render_std', 'render_count', 'render_p50', 'render_p90', 'render_p95', 'render_p99']\n\ntest_batch_stats = test_df.groupby('batch').agg({\n    'startup_latency_ms': ['mean', 'std', 'count', lambda x: x.quantile(0.5), lambda x: x.quantile(0.9), lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)],\n    'render_latency_ms': ['mean', 'std', 'count', lambda x: x.quantile(0.5), lambda x: x.quantile(0.9), lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)]\n}).reset_index()\n\n# Rename the lambda columns to proper names\ntest_batch_stats.columns = ['batch', 'startup_mean', 'startup_std', 'startup_count', 'startup_p50', 'startup_p90', 'startup_p95', 'startup_p99',\n                             'render_mean', 'render_std', 'render_count', 'render_p50', 'render_p90', 'render_p95', 'render_p99']\n\nprint('=' * 80)\nprint('BATCH-LEVEL STATISTICS')\nprint('=' * 80)\nprint('\\nBase Run - Batch Metrics:')\nprint(base_batch_stats)\nprint('\\nTest Run - Batch Metrics:')\nprint(test_batch_stats)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d3f16",
   "metadata": {},
   "outputs": [],
   "source": "# Extract batch-level statistics for analysis\nbase_startup_means = base_batch_stats['startup_mean'].values\ntest_startup_means = test_batch_stats['startup_mean'].values\nbase_render_means = base_batch_stats['render_mean'].values\ntest_render_means = test_batch_stats['render_mean'].values\n\ndef permutation_test(group1, group2, n_permutations=10000):\n    observed_diff = np.mean(group2) - np.mean(group1)\n    combined = np.concatenate([group1, group2])\n    n1 = len(group1)\n    perm_diffs = []\n    for _ in range(n_permutations):\n        np.random.shuffle(combined)\n        perm_diff = np.mean(combined[n1:]) - np.mean(combined[:n1])\n        perm_diffs.append(perm_diff)\n    p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n    return observed_diff, p_value\n\n# Calculate overall statistics including percentiles\ndef calculate_summary_stats(df, metric):\n    return {\n        'mean': df[metric].mean(),\n        'p50': df[metric].quantile(0.5),\n        'p90': df[metric].quantile(0.9),\n        'p95': df[metric].quantile(0.95),\n        'p99': df[metric].quantile(0.99)\n    }\n\n# Startup latency analysis\nstartup_diff, startup_pval = permutation_test(base_startup_means, test_startup_means)\nbase_startup_stats = calculate_summary_stats(base_df, 'startup_latency_ms')\ntest_startup_stats = calculate_summary_stats(test_df, 'startup_latency_ms')\n\nprint('\\n' + '=' * 80)\nprint('BATCH AGGREGATION RESULTS - STARTUP LATENCY')\nprint('=' * 80)\nprint(f'\\nBase Statistics:')\nprint(f'  Mean:  {base_startup_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {base_startup_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {base_startup_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {base_startup_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {base_startup_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nTest Statistics:')\nprint(f'  Mean:  {test_startup_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {test_startup_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {test_startup_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {test_startup_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {test_startup_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nDifferences (Test - Base):')\nprint(f'  Mean:  {test_startup_stats[\"mean\"] - base_startup_stats[\"mean\"]:.2f} ms ({(test_startup_stats[\"mean\"] - base_startup_stats[\"mean\"])/base_startup_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  P50:   {test_startup_stats[\"p50\"] - base_startup_stats[\"p50\"]:.2f} ms ({(test_startup_stats[\"p50\"] - base_startup_stats[\"p50\"])/base_startup_stats[\"p50\"]*100:+.2f}%)')\nprint(f'  P90:   {test_startup_stats[\"p90\"] - base_startup_stats[\"p90\"]:.2f} ms ({(test_startup_stats[\"p90\"] - base_startup_stats[\"p90\"])/base_startup_stats[\"p90\"]*100:+.2f}%)')\nprint(f'  P95:   {test_startup_stats[\"p95\"] - base_startup_stats[\"p95\"]:.2f} ms ({(test_startup_stats[\"p95\"] - base_startup_stats[\"p95\"])/base_startup_stats[\"p95\"]*100:+.2f}%)')\nprint(f'  P99:   {test_startup_stats[\"p99\"] - base_startup_stats[\"p99\"]:.2f} ms ({(test_startup_stats[\"p99\"] - base_startup_stats[\"p99\"])/base_startup_stats[\"p99\"]*100:+.2f}%)')\n\nprint(f'\\nPermutation Test (using batch means):')\nprint(f'  Difference: {startup_diff:.2f} ms ({startup_diff/np.mean(base_startup_means)*100:+.2f}%)')\nprint(f'  P-value: {startup_pval:.4f}')\nprint(f'  Significant at Î±=0.05: {\"YES\" if startup_pval < 0.05 else \"NO\"}')\n\n# Render latency analysis\nrender_diff, render_pval = permutation_test(base_render_means, test_render_means)\nbase_render_stats = calculate_summary_stats(base_df, 'render_latency_ms')\ntest_render_stats = calculate_summary_stats(test_df, 'render_latency_ms')\n\nprint('\\n' + '=' * 80)\nprint('BATCH AGGREGATION RESULTS - RENDER LATENCY')\nprint('=' * 80)\nprint(f'\\nBase Statistics:')\nprint(f'  Mean:  {base_render_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {base_render_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {base_render_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {base_render_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {base_render_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nTest Statistics:')\nprint(f'  Mean:  {test_render_stats[\"mean\"]:.2f} ms')\nprint(f'  P50:   {test_render_stats[\"p50\"]:.2f} ms')\nprint(f'  P90:   {test_render_stats[\"p90\"]:.2f} ms')\nprint(f'  P95:   {test_render_stats[\"p95\"]:.2f} ms')\nprint(f'  P99:   {test_render_stats[\"p99\"]:.2f} ms')\n\nprint(f'\\nDifferences (Test - Base):')\nprint(f'  Mean:  {test_render_stats[\"mean\"] - base_render_stats[\"mean\"]:.2f} ms ({(test_render_stats[\"mean\"] - base_render_stats[\"mean\"])/base_render_stats[\"mean\"]*100:+.2f}%)')\nprint(f'  P50:   {test_render_stats[\"p50\"] - base_render_stats[\"p50\"]:.2f} ms ({(test_render_stats[\"p50\"] - base_render_stats[\"p50\"])/base_render_stats[\"p50\"]*100:+.2f}%)')\nprint(f'  P90:   {test_render_stats[\"p90\"] - base_render_stats[\"p90\"]:.2f} ms ({(test_render_stats[\"p90\"] - base_render_stats[\"p90\"])/base_render_stats[\"p90\"]*100:+.2f}%)')\nprint(f'  P95:   {test_render_stats[\"p95\"] - base_render_stats[\"p95\"]:.2f} ms ({(test_render_stats[\"p95\"] - base_render_stats[\"p95\"])/base_render_stats[\"p95\"]*100:+.2f}%)')\nprint(f'  P99:   {test_render_stats[\"p99\"] - base_render_stats[\"p99\"]:.2f} ms ({(test_render_stats[\"p99\"] - base_render_stats[\"p99\"])/base_render_stats[\"p99\"]*100:+.2f}%)')\n\nprint(f'\\nPermutation Test (using batch means):')\nprint(f'  Difference: {render_diff:.2f} ms ({render_diff/np.mean(base_render_means)*100:+.2f}%)')\nprint(f'  P-value: {render_pval:.4f}')\nprint(f'  Significant at Î±=0.05: {\"YES\" if render_pval < 0.05 else \"NO\"}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}